{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Perform a Google search and extract the top 10 result URLs\n",
    "def get_google_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to Google search\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the top 10 results\n",
    "    result_divs = soup.find_all('div', class_='tF2Cxc', limit=10)\n",
    "    \n",
    "    urls = []\n",
    "    for result in result_divs:\n",
    "        link = result.find('a')['href']\n",
    "        urls.append(link)\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scrape each URL for text, H1, H2, H3 elements\n",
    "def scrape_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract text from <p> tags\n",
    "        text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "        \n",
    "        # Extract H1, H2, H3 headers\n",
    "        h1 = [h1.get_text() for h1 in soup.find_all('h1')]\n",
    "        h2 = [h2.get_text() for h2 in soup.find_all('h2')]\n",
    "        h3 = [h3.get_text() for h3 in soup.find_all('h3')]\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'text': text,\n",
    "            'h1': h1,\n",
    "            'h2': h2,\n",
    "            'h3': h3\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.geeksforgeeks.org/python-web-scraping-tutorial/ ...\n",
      "Scraped data from https://www.geeksforgeeks.org/python-web-scraping-tutorial/:\n",
      "H1 tags: ['Python Web Scraping Tutorial']\n",
      "H2 tags: ['Introduction to Web Scraping', 'Basics of Web Scraping', 'Setting Up the Environment', 'Extracting Data from Web Pages', 'Fetching Web Pages', 'HTTP Request Methods', 'Searching and Extract for specific tags Beautifulsoup', 'Scrapy Basics', 'Selenium Python Basics', ' Essential Packages and Tools for Python Web Scraping ', ' Requests Module ', ' BeautifulSoup Library ', ' Selenium ', ' Lxml ', ' Urllib Module ', ' PyautoGUI ', ' Schedule ', ' Why Python3 for Web Scraping? ', ' Conclusion ', 'What kind of Experience do you want to share?']\n",
      "H3 tags: [' Example: Making a Request ', ' Example ', ' Finding Elements by Class ', ' Example  1: For Firefox ', ' Example 2: For Chrome ', ' Example ', ' Example ', ' Example ', ' Example ', 'Similar Reads', 'Please Login to comment...']\n",
      "Text preview:  Web scraping, the process of extracting data from websites, has emerged as a powerful technique to gather information from the vast expanse of the internet. In this tutorial, we‚Äôll explore various Py...\n",
      "\n",
      "Scraping https://webscraper.io/tutorials ...\n",
      "Scraped data from https://webscraper.io/tutorials:\n",
      "H1 tags: ['Video Tutorials']\n",
      "H2 tags: ['Extension intro video', 'Cloud overview', 'Pagination', 'Multiple records', 'Parser']\n",
      "H3 tags: []\n",
      "Text preview: Web Scraper Cloud Scraper Pricing Learn Navigate multi-level navigation to scrape all items in an e-commerce site. Overview for most Web Scraper Cloud features. Scrape e-commerce sites with pagination...\n",
      "\n",
      "Scraping https://realpython.com/python-web-scraping-practical-introduction/ ...\n",
      "Scraped data from https://realpython.com/python-web-scraping-practical-introduction/:\n",
      "H1 tags: ['A Practical Introduction to Web Scraping in Python']\n",
      "H2 tags: ['Scrape and Parse Text From Websites', 'Use an HTML Parser for Web Scraping in Python', 'Interact With HTML Forms', 'Interact With Websites in Real Time', 'Conclusion', 'Additional Resources', 'Keep reading Real\\xa0Python by creating a free account or signing\\xa0in:']\n",
      "H3 tags: ['Build Your First Web Scraper', 'Extract Text From HTML With String Methods', 'Get to Know Regular Expressions', 'Extract Text From HTML With Regular Expressions', 'Check Your Understanding', 'Install Beautiful Soup', 'Create a BeautifulSoup Object', 'Use a BeautifulSoup Object', 'Check Your Understanding', 'Install MechanicalSoup', 'Create a Browser Object', 'Submit a Form With MechanicalSoup', 'Check Your Understanding']\n",
      "Text preview: ‚Äî FREE Email Series ‚Äî üêç Python Tricks üíå  üîí No spam. Unsubscribe any time. Table of Contents  Recommended Video CourseExercises Course: Introduction to Web Scraping With Python Table of Contents  Watch...\n",
      "\n",
      "Scraping https://www.scrapingbee.com/blog/web-scraping-101-with-python/ ...\n",
      "Scraped data from https://www.scrapingbee.com/blog/web-scraping-101-with-python/:\n",
      "H1 tags: ['Python Web Scraping: Full Tutorial With Examples (2024)']\n",
      "H2 tags: ['0. Web Scraping Process', '1. Manually Opening a Socket and Sending the HTTP Request', '2. Using Urllib3 & LXML', '3. Using Requests & BeautifulSoup', '4. Using Web Crawling Frameworks', '5. Using Headless Browsing', '6. Using a Website‚Äôs API', '7. Avoiding Anti-Bot Technology', 'Conclusion', 'Ready to Level Up Your Scraping Game? Check Out ScrapingBee!', 'Tired of getting blocked while scraping the web?']\n",
      "H3 tags: ['Socket', 'Regular Expressions', 'Urllib3', 'XPath', 'Requests', 'BeautifulSoup', 'Storing our data in CSV', 'Storing Our Data in PostgreSQL', 'Summary', 'Asyncio', 'Scrapy', 'PySpider', 'PySpider vs. Scrapy: A Quick Comparison', 'Selenium & Chrome', 'RoboBrowser', 'Scraping a Website‚Äôs Content API', 'Scraping Reddit Data', 'Using Proxies', 'Setting or Rotating User Agents', 'Undetected ChromeDriver', 'No Driver', 'Balancing Act', 'Further Reading', ' You might also like:']\n",
      "Text preview: Have you ever wondered how to gather data from websites automatically? Or how some websites and web applications can extract and display data so seamlessly from other sites in real-time? Whether you w...\n",
      "\n",
      "Scraping https://oxylabs.io/blog/python-web-scraping ...\n",
      "Scraped data from https://oxylabs.io/blog/python-web-scraping:\n",
      "H1 tags: ['Python Web Scraping Tutorial: Step-By-Step']\n",
      "H2 tags: ['Building a web scraper: Python prepwork', 'Getting to the libraries', 'WebDrivers and browsers', 'Finding a cozy place for our Python web scraper', 'Importing and using libraries', 'Picking a URL', 'Defining objects and building lists', 'Extracting data with a Python web scraper', 'Exporting the data to CSV', 'Exporting the data to Excel', 'More lists. More!', 'Web scraping with Python best practices', 'Conclusion', 'Frequently Asked Questions']\n",
      "H3 tags: ['Requests library', 'Beautiful Soup', 'lxml', 'Selenium', 'Web scraping Python libraries compared', 'Is Python good for web scraping?', 'Is Python better than R for web scraping?', 'What are the disadvantages of web scraping in Python?', 'Is Selenium better than BeautifulSoup?', 'Related articles']\n",
      "Text preview: Proxy locations Europe North America South America Asia Africa Oceania See all locations English (EN) English ‰∏≠Êñá Log in Back to blog Adomas Sulcas Getting started in web scraping is simple except when...\n",
      "\n",
      "Scraping https://sdlccorp.com/post/step-by-step-tutorial-for-web-scraping-with-python/ ...\n",
      "Scraped data from https://sdlccorp.com/post/step-by-step-tutorial-for-web-scraping-with-python/:\n",
      "H1 tags: ['Step-by-step tutorial for web scraping with Python.', 'Step-by-step tutorial for web scraping with Python.']\n",
      "H2 tags: ['Related Posts', 'How to do a Web Scraping with Python?', 'Overview', 'How its work?', 'What are the different Python web scraping libraries?', 'How to scrape data from websites using Python?', 'How to parse text from the website?', 'How to scrape HTML forms using Python?', 'How to scrape HTML forms using Python?', 'Benefits of Web Scraping with', 'Conclusion', 'FAQs', 'Contact Us', 'Contact Us', 'Top Categories', 'Contact Us']\n",
      "H3 tags: ['  Fantasy Sports App ', '  Procedural Generation: Unlocking Infinite Worlds in Gaming ', '  Top Blockchain Game Development Companies in Dubai, UAE in 2024 ', '  Top 10 Online Bitcoin Casino Games ', '  The Role of Virtual Reality (VR) in Android Game Development ', '  Unreal Engine for Indie Game Developers: Advantages and Tips ', 'Python Tutorial', '     1. What is the best Python library for extracting table data from PDFs?', '     2. How can I handle PDFs with complex layouts or multiple tables?', '     3. Is web scraping legal?', '     4. Can I scrape dynamic content or websites with JavaScript using Python?', '     5. How can I avoid being blocked while scraping data from websites?', ' Request a Quote ']\n",
      "Text preview: Explore Our Other Insights! Web scraping with Python is a powerful technique used to extract data from websites. In this step-by-step tutorial, we‚Äôll guide you through the process of setting up your P...\n",
      "\n",
      "Scraping https://www.tutorialspoint.com/python_web_scraping/index.htm ...\n",
      "Scraped data from https://www.tutorialspoint.com/python_web_scraping/index.htm:\n",
      "H1 tags: ['Python Web Scraping Tutorial', 'Audience', 'Prerequisites']\n",
      "H2 tags: []\n",
      "H3 tags: []\n",
      "Text preview: Web scraping, also called web data mining or web harvesting, is the process of constructing an agent which can extract, parse, download and organize useful information from the web automatically. This...\n",
      "\n",
      "Scraping https://nanonets.com/blog/web-scraping-with-python-tutorial/ ...\n",
      "Scraped data from https://nanonets.com/blog/web-scraping-with-python-tutorial/:\n",
      "H1 tags: ['Web Scraping with Python Tutorial']\n",
      "H2 tags: ['What is Web Scraping?', 'What are the different Python web scraping libraries?', 'How to scrape data from websites using Python?', 'How to parse text from the website?', 'How to scrape HTML forms using Python?', 'Comparing all Python web scraping libraries', 'Conclusion', 'FAQs', '\\r\\n                        Related content\\r\\n                    ']\n",
      "H3 tags: []\n",
      "Text preview:  Suppose you want to scrape competitor websites for their pricing page information. What will you do? Copy-pasting or entering data manually is too slow, time-consuming, and error-prone. You can autom...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Combine the Google search and page scraping\n",
    "def main(query):\n",
    "    # Get the top 10 search results from Google\n",
    "    urls = get_google_search_results(query)\n",
    "    \n",
    "    # Scrape each URL\n",
    "    for url in urls:\n",
    "        print(f\"Scraping {url} ...\")\n",
    "        page_data = scrape_page(url)\n",
    "        \n",
    "        if page_data:\n",
    "            print(f\"Scraped data from {url}:\")\n",
    "            print(f\"H1 tags: {page_data['h1']}\")\n",
    "            print(f\"H2 tags: {page_data['h2']}\")\n",
    "            print(f\"H3 tags: {page_data['h3']}\")\n",
    "            print(f\"Text preview: {page_data['text'][:200]}...\\n\")\n",
    "        else:\n",
    "            print(f\"Skipping {url} due to an error.\")\n",
    "\n",
    "# Example usage\n",
    "main(\"web scraping tutorial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
